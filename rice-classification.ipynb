{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3399185,"sourceType":"datasetVersion","datasetId":2049052}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/babaksh/rice-classification?scriptVersionId=223228816\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Rice Image Dataset\n\n## **Overview**\nThe **Rice Image Dataset** is a collection of high-resolution images designed for rice grain classification. It contains images of five different rice varieties, making it useful for machine learning applications in agricultural research, food quality assessment, and automated classification systems.\n\n## **Dataset Details**\n- **Total Images**: 75,000  \n- **Number of Classes**: 5  \n- **Image Size**: 250 × 250 pixels  \n- **Format**: JPEG  \n- **Grayscale or RGB**: RGB  \n\n## **Classes (Rice Varieties)**\nThe dataset includes images of the following five types of rice grains:  \n1. **Arborio**  \n2. **Basmati**  \n3. **Ipsala**  \n4. **Jasmine**  \n5. **Karacadag**  \n\nEach class contains an equal number of images, ensuring balanced data for training machine learning models.\n\n## **Potential Use Cases**\n- **Classification Models**: Train deep learning models (e.g., CNNs, ViTs) to classify rice varieties.\n- **Feature Extraction**: Extract texture, shape, and color features for distinguishing different rice types.\n- **Quality Assessment**: Identify high-quality vs. low-quality grains using automated inspection systems.\n- **Agricultural Research**: Analyze grain characteristics to improve production and processing techniques.\n","metadata":{}},{"cell_type":"markdown","source":"# Code Overview\n\nThis code imports essential libraries for building and training an image classification model using TensorFlow and Keras.\n\n- **os, numpy, pandas**: For data management and numerical operations.\n- **seaborn, matplotlib**: For data visualization and plotting.\n- **tensorflow**: Core framework for deep learning tasks, including model building and training.\n- **sklearn**: For splitting data and dimensionality reduction (t-SNE).\n- **ImageDataGenerator**: For real-time data augmentation.\n- **Callbacks (EarlyStopping, ModelCheckpoint)**: To improve training by monitoring performance.\n\nThese libraries help preprocess data, define the model, optimize training, and visualize results.\n","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Resizing\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.applications import ResNet50\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import image_dataset_from_directory\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Flatten\nfrom tensorflow.keras.layers import Activation, MaxPooling2D, GlobalAveragePooling2D, Dropout, Dense","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:16:13.053317Z","iopub.execute_input":"2025-02-18T14:16:13.053555Z","iopub.status.idle":"2025-02-18T14:16:27.234439Z","shell.execute_reply.started":"2025-02-18T14:16:13.053532Z","shell.execute_reply":"2025-02-18T14:16:27.233478Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Image Parameters and Dataset Path\n\nIn this step, we define key parameters for preprocessing and model training:\n\n- **Image Size**: Resized to `(224, 224)` for consistency across the dataset.\n- **Channels**: Set to `3` (RGB), ensuring compatibility with deep learning models.\n- **Batch Size**: `64`, balancing memory usage and training efficiency.\n- **Epochs**: `15`, defining the number of training iterations.\n- **Learning Rate**: `1e-3`, a standard starting point for optimization.\n- **Patience**: `10`, allowing early stopping if validation performance does not improve.\n\nThe dataset path is set to the **Rice Image Dataset** location on Kaggle.","metadata":{}},{"cell_type":"code","source":"# Image parameters\nIMG_SIZE = (224, 224)\nCHANNELS = 3\nBATCH_SIZE = 64\nEPOCHS = 15\nLEARNING_RATE = 1e-3\nPATIENCE = 10\n\n# Paths\ndataset_path = \"/kaggle/input/rice-image-dataset/Rice_Image_Dataset\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:16:27.235483Z","iopub.execute_input":"2025-02-18T14:16:27.236054Z","iopub.status.idle":"2025-02-18T14:16:27.24018Z","shell.execute_reply.started":"2025-02-18T14:16:27.236028Z","shell.execute_reply":"2025-02-18T14:16:27.239247Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Preparation: Train, Validation, and Test Generators\n\nThis step loads the **Rice Image Dataset** and splits it into **training, validation, and test sets** using `image_dataset_from_directory()`.\n\n- **Training Set** (`train_generator`): \n  - 80% of the dataset.\n  - Labels are inferred and one-hot encoded (`categorical` mode).\n  - Images are resized to `(224, 224)`.\n  - Batch size is `64`.\n\n- **Validation Set** (`val_generator`):  \n  - 20% of the dataset.\n  - Used to monitor model performance during training.\n\n- **Test Set** (`test_generator`):  \n  - 10% of the dataset, separate from validation.\n  - Used for final evaluation.\n\nA **random seed (1000)** ensures reproducibility of splits.","metadata":{}},{"cell_type":"code","source":"train_generator = image_dataset_from_directory(\n    directory=dataset_path,\n    labels='inferred',\n    subset='training',\n    label_mode='categorical',\n    batch_size=BATCH_SIZE,\n    image_size=IMG_SIZE,\n    validation_split=0.2,\n    seed=1000\n)\n\nval_generator = image_dataset_from_directory(\n    directory=dataset_path,\n    subset='validation',\n    labels='inferred',\n    label_mode='categorical',\n    batch_size=BATCH_SIZE,\n    image_size=IMG_SIZE,\n    validation_split=0.2,\n    seed=1000\n)\n\ntest_generator = image_dataset_from_directory(\n    directory=dataset_path,\n    labels='inferred',\n    subset='validation',\n    label_mode='categorical',\n    batch_size=BATCH_SIZE,\n    image_size=IMG_SIZE,\n    validation_split=0.1,\n    seed=1000\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:16:27.241829Z","iopub.execute_input":"2025-02-18T14:16:27.242116Z","iopub.status.idle":"2025-02-18T14:17:54.555442Z","shell.execute_reply.started":"2025-02-18T14:16:27.24209Z","shell.execute_reply":"2025-02-18T14:17:54.554788Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CNN Model Architecture\n\nThis Convolutional Neural Network (CNN) is designed for image classification and consists of three convolutional blocks followed by a classification block.\n\n- **First Convolutional Block**: \n  - Uses a 32-filter `Conv2D` layer with a `3x3` kernel and ReLU activation.\n  - Applies `BatchNormalization` to stabilize training.\n  - Uses `MaxPooling2D` to reduce spatial dimensions.\n  - Includes `Dropout (0.3)` to prevent overfitting.\n\n- **Second Convolutional Block**:\n  - Expands to 64 filters with similar layers as the first block.\n  - `Dropout (0.4)` is used for regularization.\n\n- **Third Convolutional Block**:\n  - Uses 128 filters, `BatchNormalization`, and ReLU activation.\n  - `MaxPooling2D` downsamples features.\n  - `Dropout (0.5)` helps prevent overfitting.\n\n- **Classification Block**:\n  - `GlobalAveragePooling2D` followed by `Flatten` reduces feature maps.\n  - Fully connected (`Dense`) layers progressively reduce dimensions: `512 → 256 → 128`.\n  - `BatchNormalization` and `Dropout` (up to 0.7) improve generalization.\n  - Final `Dense` layer with `softmax` activation outputs class probabilities.\n\nThis model balances depth and regularization for robust feature extraction and classification.","metadata":{}},{"cell_type":"code","source":"# Build the CNN model\ndef build_cnn_model(input_shape, num_classes):\n    model = Sequential([\n        # First Convolutional Block\n        Conv2D(32, (3, 3), padding='same', input_shape=input_shape),\n        BatchNormalization(),\n        Activation('relu'),\n        MaxPooling2D((2, 2)),\n        Dropout(0.3),\n        \n        # Second Convolutional Block\n        Conv2D(64, (3, 3), padding='same'),\n        BatchNormalization(),\n        Activation('relu'),\n        MaxPooling2D((2, 2)),\n        Dropout(0.4),\n        \n        # Third Convolutional Block\n        Conv2D(128, (3, 3), padding='same'),\n        BatchNormalization(),\n        Activation('relu'),\n        MaxPooling2D((2, 2)),\n        Dropout(0.5),\n        \n        # Classification Block\n        GlobalAveragePooling2D(),\n        Flatten(),\n        Dense(512, activation='relu'),\n        Dropout(0.6),\n        Dense(256, activation='relu'),\n        Dropout(0.65),\n        Dense(128, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.7),\n        Dense(num_classes, activation='softmax')\n    ])\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:17:54.556469Z","iopub.execute_input":"2025-02-18T14:17:54.556671Z","iopub.status.idle":"2025-02-18T14:17:54.562077Z","shell.execute_reply.started":"2025-02-18T14:17:54.556654Z","shell.execute_reply":"2025-02-18T14:17:54.561234Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# AlexNet Model Architecture\n\nThis function implements a variant of the **AlexNet** architecture for image classification, modified for use with TensorFlow/Keras.\n\n- **Input & Preprocessing**:\n  - The input is resized to `227x227` to match the original AlexNet dimensions.\n\n- **Feature Extraction (Convolutional Blocks)**:\n  - **Block 1**: Uses a `96-filter` convolutional layer with an `11x11` kernel, `stride 4`, followed by `MaxPooling` and `BatchNormalization`.\n  - **Block 2**: Expands to `256 filters` with a `5x5` kernel, `same padding`, followed by `MaxPooling` and `BatchNormalization`.\n  - **Block 3**: Uses `384 filters` with a `3x3` kernel.\n  - **Block 4**: Another `384-filter` convolutional layer with a `3x3` kernel.\n  - **Block 5**: Uses `256 filters` with a `3x3` kernel, followed by `MaxPooling`.\n\n- **Classification (Fully Connected Layers)**:\n  - `GlobalAveragePooling2D` and `Flatten` prepare features for dense layers.\n  - Two fully connected layers with `4096` neurons and `ReLU` activation.\n  - `Dropout (0.5)` helps prevent overfitting.\n  - A final `Dense` layer with `softmax` activation outputs class probabilities.\n\nThis architecture is optimized for large-scale image classification, leveraging deep convolutional layers and fully connected layers for robust feature learning.","metadata":{}},{"cell_type":"code","source":"def build_alexnet_model(input_shape, num_classes):\n    model = tf.keras.Sequential([\n        Input(shape=input_shape),\n        tf.keras.layers.Resizing(227, 227),\n        \n        # Block 1\n        Conv2D(96, (11, 11), strides=4, activation='relu'),\n        MaxPooling2D((3, 3), strides=2),\n        BatchNormalization(),\n        \n        # Block 2\n        Conv2D(256, (5, 5), padding='same', activation='relu'),\n        MaxPooling2D((3, 3), strides=2),\n        BatchNormalization(),\n        \n        # Block 3\n        Conv2D(384, (3, 3), padding='same', activation='relu'),\n        \n        # Block 4\n        Conv2D(384, (3, 3), padding='same', activation='relu'),\n        \n        # Block 5\n        Conv2D(256, (3, 3), padding='same', activation='relu'),\n        MaxPooling2D((3, 3), strides=2),\n        \n        # Classifier\n        GlobalAveragePooling2D(),\n        Flatten(),\n        Dense(4096, activation='relu'),\n        Dropout(0.5),\n        Dense(4096, activation='relu'),\n        Dropout(0.5),\n        Dense(num_classes, activation='softmax')\n    ])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:17:54.56285Z","iopub.execute_input":"2025-02-18T14:17:54.563099Z","iopub.status.idle":"2025-02-18T14:17:54.585499Z","shell.execute_reply.started":"2025-02-18T14:17:54.563068Z","shell.execute_reply":"2025-02-18T14:17:54.584911Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ResNet50 Model Architecture\n\nThis function builds a **ResNet50-based** model for image classification using transfer learning.\n\n- **Base Model (ResNet50)**:\n  - Loads a pre-trained `ResNet50` model with `ImageNet` weights.\n  - `include_top=False` removes the original fully connected layers.\n  - `trainable=False` freezes the base model to retain learned features.\n\n- **Preprocessing & Feature Extraction**:\n  - Input is resized to `224x224` to match ResNet50's expected dimensions.\n  - Uses `tf.keras.applications.resnet50.preprocess_input()` for standardization.\n  - The base model extracts deep hierarchical features.\n\n- **Classification Head**:\n  - `GlobalAveragePooling2D` reduces the feature maps to a single vector.\n  - A `Dense (256 units, ReLU)` layer captures high-level representations.\n  - `Dropout (0.5)` prevents overfitting.\n  - The final `Dense` layer with `softmax` activation outputs class probabilities.\n\nThis approach leverages **ResNet50's** powerful feature extraction while adding a custom classification head for adaptation to new datasets.\n","metadata":{}},{"cell_type":"code","source":"def build_resnet50_model(input_shape, num_classes):\n    base_model = ResNet50(\n        weights='imagenet', \n        include_top=False, \n        input_shape=(224, 224, 3)\n    )\n    base_model.trainable = False  \n\n    # Build new model\n    inputs = Input(shape=input_shape)\n    x = Resizing(224, 224)(inputs)  # Resize to 224x224\n    x = tf.keras.applications.resnet50.preprocess_input(x)\n    x = base_model(x)\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    outputs = Dense(num_classes, activation='softmax')(x)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:17:54.586212Z","iopub.execute_input":"2025-02-18T14:17:54.586473Z","iopub.status.idle":"2025-02-18T14:17:54.602313Z","shell.execute_reply.started":"2025-02-18T14:17:54.586426Z","shell.execute_reply":"2025-02-18T14:17:54.601519Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Determining Number of Classes and Input Shape\n\n- **Number of Classes**: Extracted from `train_generator.class_names`, representing the five rice grain categories in the dataset.  \n- **Class Names**: Stored in `classes`, listing all unique categories.  \n- **Input Shape**: Defined as `(224, 224, 3)`, matching the resized image dimensions and RGB channels.\n\nThese values will be used to configure the model architecture for classification.\n","metadata":{}},{"cell_type":"code","source":"# Determine the number of classes from the training generator\nnum_classes = len(train_generator.class_names)\nclasses = train_generator.class_names\ninput_shape = (IMG_SIZE[0], IMG_SIZE[1], CHANNELS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:17:54.603176Z","iopub.execute_input":"2025-02-18T14:17:54.603405Z","iopub.status.idle":"2025-02-18T14:17:54.617147Z","shell.execute_reply.started":"2025-02-18T14:17:54.603387Z","shell.execute_reply":"2025-02-18T14:17:54.616232Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Building and Compiling CNN Models\n\nIn this step, three deep learning models are built and compiled for rice image classification:\n\n1. **Custom CNN Model (`cnn_model`)**:\n   - A sequential CNN architecture with multiple convolutional layers.\n   - Uses `ReLU` activation, `BatchNormalization`, and `Dropout` for regularization.\n\n2. **AlexNet Model (`alexnet_model`)**:\n   - Based on the **AlexNet** architecture, optimized for image classification.\n   - Includes large fully connected layers and `Dropout` to prevent overfitting.\n\n3. **ResNet50 Model (`resnet50_model`)**:\n   - Utilizes **ResNet50** with pre-trained `ImageNet` weights.\n   - Freezes the base model for feature extraction and adds a custom classification head.\n\nEach model is **compiled** with:\n- **Optimizer**: `Adam` with a learning rate of `1e-3` for efficient training.\n- **Loss Function**: `Categorical Crossentropy`, suitable for multi-class classification.\n- **Evaluation Metric**: `Accuracy`, to assess model performance.\n\nModel summaries are displayed to review the architecture and parameter count.\n","metadata":{}},{"cell_type":"code","source":"# Build and compile the cnn model\ncnn_model = build_cnn_model(input_shape, num_classes)\n\ncnn_model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\"])\n\ncnn_model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:17:54.619517Z","iopub.execute_input":"2025-02-18T14:17:54.61976Z","iopub.status.idle":"2025-02-18T14:17:55.860051Z","shell.execute_reply.started":"2025-02-18T14:17:54.619715Z","shell.execute_reply":"2025-02-18T14:17:55.859392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build and compile the alexnet model\nalexnet_model = build_alexnet_model(input_shape, num_classes)\n\nalexnet_model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\"])\n\nalexnet_model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:17:55.861384Z","iopub.execute_input":"2025-02-18T14:17:55.861612Z","iopub.status.idle":"2025-02-18T14:17:55.975436Z","shell.execute_reply.started":"2025-02-18T14:17:55.861594Z","shell.execute_reply":"2025-02-18T14:17:55.97482Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build and compile the resnet model\nresnet50_model = build_resnet50_model(input_shape, num_classes)\n\nresnet50_model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\"])\n\nresnet50_model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:17:55.976226Z","iopub.execute_input":"2025-02-18T14:17:55.976502Z","iopub.status.idle":"2025-02-18T14:17:57.760313Z","shell.execute_reply.started":"2025-02-18T14:17:55.976476Z","shell.execute_reply":"2025-02-18T14:17:57.759642Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Defining Callbacks for Model Training\n\nTo enhance training efficiency and prevent overfitting, two callbacks are defined:\n\n1. **Early Stopping (`early_stopping`)**:\n   - Monitors **validation loss (`val_loss`)**.\n   - Stops training if no improvement is observed for `PATIENCE` epochs (set to 10).\n   - Restores the best model weights to avoid overfitting.\n\n2. **Model Checkpointing (`model_checkpoint`)**:\n   - Saves the best model (`best_model.keras`) based on **validation loss**.\n   - Ensures only the most optimal model is retained for further evaluation.\n\nThese callbacks help improve generalization and reduce unnecessary training time.\n","metadata":{}},{"cell_type":"code","source":"# Define callbacks for early stopping and model checkpointing\nearly_stopping_cnn = EarlyStopping(\n    monitor='val_loss', \n    patience=PATIENCE, \n    restore_best_weights=True,\n    verbose=1\n)\n\nmodel_checkpoint_cnn = ModelCheckpoint(\n    'best_cnn.keras', \n    monitor='val_loss', \n    save_best_only=True,\n    verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:17:57.761087Z","iopub.execute_input":"2025-02-18T14:17:57.761397Z","iopub.status.idle":"2025-02-18T14:17:57.765552Z","shell.execute_reply.started":"2025-02-18T14:17:57.761364Z","shell.execute_reply":"2025-02-18T14:17:57.764642Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"early_stopping_alexnet = EarlyStopping(\n    monitor='val_loss', \n    patience=PATIENCE, \n    restore_best_weights=True,\n    verbose=1\n)\n\nmodel_checkpoint_alexnet = ModelCheckpoint(\n    'best_alexnet.keras', \n    monitor='val_loss', \n    save_best_only=True,\n    verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:17:57.766336Z","iopub.execute_input":"2025-02-18T14:17:57.766546Z","iopub.status.idle":"2025-02-18T14:17:57.782777Z","shell.execute_reply.started":"2025-02-18T14:17:57.766528Z","shell.execute_reply":"2025-02-18T14:17:57.782119Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"early_stopping_resnet = EarlyStopping(\n    monitor='val_loss', \n    patience=PATIENCE, \n    restore_best_weights=True,\n    verbose=1\n)\n\nmodel_checkpoint_resnet = ModelCheckpoint(\n    'best_resnet.keras', \n    monitor='val_loss', \n    save_best_only=True,\n    verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:17:57.783522Z","iopub.execute_input":"2025-02-18T14:17:57.783707Z","iopub.status.idle":"2025-02-18T14:17:57.797391Z","shell.execute_reply.started":"2025-02-18T14:17:57.78369Z","shell.execute_reply":"2025-02-18T14:17:57.796745Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training the CNN, AlexNet, and ResNet50 Models\n\nEach model is trained using the **fit()** function with the following settings:\n\n- **Training Data**: `train_generator`\n- **Validation Data**: `val_generator`\n- **Epochs**: `20` (or until early stopping is triggered)\n- **Callbacks**:\n  - **Early Stopping**: Stops training if validation loss does not improve for `PATIENCE` epochs.\n  - **Model Checkpoint**: Saves the best model based on validation loss.\n\nThe training process generates history objects (`cnn_history`, `alexnet_history`, and `resnet_history`), which store performance metrics like loss and accuracy for further analysis.\n","metadata":{}},{"cell_type":"code","source":"cnn_history = cnn_model.fit(\n    train_generator,\n    validation_data=val_generator,\n    epochs=EPOCHS,\n    callbacks=[early_stopping_cnn, model_checkpoint_cnn]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:17:57.798018Z","iopub.execute_input":"2025-02-18T14:17:57.798196Z","iopub.status.idle":"2025-02-18T14:17:57.812152Z","shell.execute_reply.started":"2025-02-18T14:17:57.79818Z","shell.execute_reply":"2025-02-18T14:17:57.81139Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"alexnet_history = alexnet_model.fit(\n    train_generator,\n    validation_data=val_generator,\n    epochs=EPOCHS,\n    callbacks=[early_stopping_alexnet, model_checkpoint_alexnet]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:17:57.812924Z","iopub.execute_input":"2025-02-18T14:17:57.813192Z","iopub.status.idle":"2025-02-18T14:17:57.826109Z","shell.execute_reply.started":"2025-02-18T14:17:57.81316Z","shell.execute_reply":"2025-02-18T14:17:57.825497Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resnet_history = resnet50_model.fit(\n    train_generator,\n    validation_data=val_generator,\n    epochs=EPOCHS,\n    callbacks=[early_stopping_resnet, model_checkpoint_resnet]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:17:57.826934Z","iopub.execute_input":"2025-02-18T14:17:57.827208Z","iopub.status.idle":"2025-02-18T14:23:02.154533Z","shell.execute_reply.started":"2025-02-18T14:17:57.82718Z","shell.execute_reply":"2025-02-18T14:23:02.153529Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Evaluation on Test Set\n\nEach trained model is evaluated on the **test set** (`test_generator`) to assess its final performance:\n\n- **CNN Model**: The test accuracy is printed for the custom CNN model.\n- **AlexNet Model**: The test accuracy is printed for the AlexNet model.\n- **ResNet50 Model**: The test accuracy is printed for the ResNet50 model.\n\nThese evaluations help determine the generalization capability of each model on unseen data.","metadata":{}},{"cell_type":"code","source":"# Evaluate the cnn model on the test set\ntest_loss, test_acc = cnn_model.evaluate(test_generator)\nprint(\"Test accuracy of Simple CNN Model:\", test_acc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:23:02.155601Z","iopub.execute_input":"2025-02-18T14:23:02.155928Z","iopub.status.idle":"2025-02-18T14:23:02.159323Z","shell.execute_reply.started":"2025-02-18T14:23:02.155903Z","shell.execute_reply":"2025-02-18T14:23:02.15855Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the alexnet model on the test set\ntest_loss, test_acc = alexnet_model.evaluate(test_generator)\nprint(\"Test accuracy of Alexnet Model:\", test_acc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:23:02.160175Z","iopub.execute_input":"2025-02-18T14:23:02.160386Z","iopub.status.idle":"2025-02-18T14:23:02.175585Z","shell.execute_reply.started":"2025-02-18T14:23:02.160367Z","shell.execute_reply":"2025-02-18T14:23:02.174926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the resnet model on the test set\ntest_loss, test_acc = resnet50_model.evaluate(test_generator)\nprint(\"Test accuracy of ResNet50 Model:\", test_acc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:23:02.176373Z","iopub.execute_input":"2025-02-18T14:23:02.176663Z","iopub.status.idle":"2025-02-18T14:23:13.890447Z","shell.execute_reply.started":"2025-02-18T14:23:02.176641Z","shell.execute_reply":"2025-02-18T14:23:13.889802Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training and Test Accuracy/Loss Plots\n\nThese plots visualize the training and test accuracy, as well as the training and test loss, for the three models:\n\n1. **Simple CNN Model**:\n   - The first set of plots shows **accuracy** and **loss** for both training and test data across epochs.\n\n2. **AlexNet Model**:\n   - The second set of plots shows the **accuracy** and **loss** for the AlexNet model during training and testing.\n\n3. **ResNet50 Model**:\n   - The third set of plots displays **accuracy** and **loss** for the ResNet50 model.\n\nThese visualizations help assess model performance, check for overfitting, and compare how each model improves over epochs.\n","metadata":{}},{"cell_type":"code","source":"# Plot training and test accuracy\nplt.figure()\nplt.plot(cnn_history.history['accuracy'], label='Training Accuracy')\nplt.plot(cnn_history.history['val_accuracy'], label='Test Accuracy')\nplt.title('Training and Test Accuracy - Simple CNN')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n# Plot training and test loss\nplt.figure()\nplt.plot(cnn_history.history['loss'], label='Training Loss')\nplt.plot(cnn_history.history['val_loss'], label='Test Loss')\nplt.title('Training and Test Loss -  Simple CNN')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:23:13.891167Z","iopub.execute_input":"2025-02-18T14:23:13.891426Z","iopub.status.idle":"2025-02-18T14:23:13.895006Z","shell.execute_reply.started":"2025-02-18T14:23:13.891393Z","shell.execute_reply":"2025-02-18T14:23:13.894025Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot training and test accuracy\nplt.figure()\nplt.plot(alexnet_history.history['accuracy'], label='Training Accuracy')\nplt.plot(alexnet_history.history['val_accuracy'], label='Test Accuracy')\nplt.title('Training and Test Accuracy - AlexNet')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n# Plot training and test loss\nplt.figure()\nplt.plot(alexnet_history.history['loss'], label='Training Loss')\nplt.plot(alexnet_history.history['val_loss'], label='Test Loss')\nplt.title('Training and Test Loss -  AlexNet')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:23:13.895683Z","iopub.execute_input":"2025-02-18T14:23:13.89589Z","iopub.status.idle":"2025-02-18T14:23:13.913033Z","shell.execute_reply.started":"2025-02-18T14:23:13.895872Z","shell.execute_reply":"2025-02-18T14:23:13.912285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot training and test accuracy\nplt.figure()\nplt.plot(resnet_history.history['accuracy'], label='Training Accuracy')\nplt.plot(resnet_history.history['val_accuracy'], label='Test Accuracy')\nplt.title('Training and Test Accuracy - ResNet50')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n# Plot training and test loss\nplt.figure()\nplt.plot(resnet_history.history['loss'], label='Training Loss')\nplt.plot(resnet_history.history['val_loss'], label='Test Loss')\nplt.title('Training and Test Loss -  ResNet50')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:23:13.913687Z","iopub.execute_input":"2025-02-18T14:23:13.913923Z","iopub.status.idle":"2025-02-18T14:23:14.367455Z","shell.execute_reply.started":"2025-02-18T14:23:13.913904Z","shell.execute_reply":"2025-02-18T14:23:14.366544Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# t-SNE Visualization for Model Feature Maps\n\nThis step visualizes the learned feature representations from each model using **t-SNE** (t-Distributed Stochastic Neighbor Embedding) to reduce high-dimensional feature maps to 2D for better understanding:\n\n- **Feature Extraction**: \n  - Features are extracted from the global average pooling layer of each model (`gap_name` parameter).\n  \n- **t-SNE Transformation**: \n  - The high-dimensional features are transformed into 2D using t-SNE, making it easier to visualize and understand how the models cluster data.\n\n- **Visualization**:\n  - **Scatter plots** display the data points in 2D space, with colors corresponding to different classes.\n\nPlots are generated for:\n1. **CNN Model**: Feature maps extracted from `global_average_pooling2d`.\n2. **AlexNet Model**: Feature maps from `global_average_pooling2d_1`.\n3. **ResNet50 Model**: Feature maps from `global_average_pooling2d_2`.\n\nThese visualizations help assess the separability of the classes based on the learned features.\n","metadata":{}},{"cell_type":"code","source":"cnn_model = tf.keras.models.load_model(\"best_cnn.keras\")\nalexnet_model = tf.keras.models.load_model(\"best_alexnet.keras\")\nresnet50_model = tf.keras.models.load_model(\"best_resnet.keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:23:14.370436Z","iopub.execute_input":"2025-02-18T14:23:14.370687Z","iopub.status.idle":"2025-02-18T14:23:15.523186Z","shell.execute_reply.started":"2025-02-18T14:23:14.370665Z","shell.execute_reply":"2025-02-18T14:23:15.522477Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_features_and_labels(model, generator):\n    features = []\n    labels = []\n\n    for batch_images, batch_labels in generator:\n        features_batch = model.predict(batch_images)  # Use the model to extract features\n        features.append(features_batch)\n        labels.append(np.argmax(batch_labels, axis=-1))  # Convert one-hot to integer labels\n\n    features = np.concatenate(features, axis=0)\n    labels = np.concatenate(labels, axis=0)\n    \n    return features, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:17:42.452436Z","iopub.execute_input":"2025-02-18T15:17:42.45275Z","iopub.status.idle":"2025-02-18T15:17:42.457511Z","shell.execute_reply.started":"2025-02-18T15:17:42.452698Z","shell.execute_reply":"2025-02-18T15:17:42.456518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_tsne(model, gap_name, title):\n\n    features, labels = extract_features_and_labels(model, test_generator)\n    \n    tsne = TSNE(n_components=2, random_state=42, perplexity=50)\n    features = StandardScaler().fit_transform(features)\n    features_tsne = tsne.fit_transform(features)\n\n    plt.figure()\n    sns.scatterplot(\n        x=features_tsne[:, 0],\n        y=features_tsne[:, 1],\n        hue=labels,\n        palette='viridis',\n        alpha=0.7\n    )\n    \n    plt.title(f\"t-SNE Visualization for {title}\")\n    plt.xlabel(\"t-SNE Component 1\")\n    plt.ylabel(\"t-SNE Component 2\")\n    plt.legend(title='Class')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:20:02.785926Z","iopub.execute_input":"2025-02-18T15:20:02.78625Z","iopub.status.idle":"2025-02-18T15:20:02.791898Z","shell.execute_reply.started":"2025-02-18T15:20:02.786221Z","shell.execute_reply":"2025-02-18T15:20:02.790969Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_tsne(cnn_model, \"global_average_pooling2d\", \"CNN Model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:23:15.548149Z","iopub.execute_input":"2025-02-18T14:23:15.548401Z","iopub.status.idle":"2025-02-18T14:23:15.565301Z","shell.execute_reply.started":"2025-02-18T14:23:15.548381Z","shell.execute_reply":"2025-02-18T14:23:15.564609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_tsne(alexnet_model, \"global_average_pooling2d_1\", \"AlexNet Model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:23:15.56617Z","iopub.execute_input":"2025-02-18T14:23:15.566433Z","iopub.status.idle":"2025-02-18T14:23:15.580206Z","shell.execute_reply.started":"2025-02-18T14:23:15.566412Z","shell.execute_reply":"2025-02-18T14:23:15.579565Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_tsne(resnet50_model, \"global_average_pooling2d_2\", \"ResNet50 Model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:20:05.97861Z","iopub.execute_input":"2025-02-18T15:20:05.979086Z","iopub.status.idle":"2025-02-18T15:20:54.37097Z","shell.execute_reply.started":"2025-02-18T15:20:05.979057Z","shell.execute_reply":"2025-02-18T15:20:54.370041Z"}},"outputs":[],"execution_count":null}]}